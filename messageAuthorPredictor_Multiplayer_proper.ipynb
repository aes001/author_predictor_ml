{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some notes on how to set up GPU for tensorflow in Jupyter notebook\n",
    "#%env PATH=\"/home/aes/Datamining/.venv/bin:/usr/local/lib/nodejs/node-v20.11.0-linux-x64/bin:/home/aes/.local/bin:/usr/local/cuda-12.4/bin:/usr/java/jdk-21.0.2/bin:/opt/gradle/gradle-8.5/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files/NVIDIA/CUDNN/v9.1/bin:/mnt/c/Python311/Scripts/:/mnt/c/Python311/:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0/:/mnt/c/Windows/System32/OpenSSH/:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/Calibre2/:/mnt/c/Program Files/dotnet/:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/nodejs/:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/MySQL/MySQL Server 8.0/bin:/mnt/c/Program Files/MySQL/MySQL Router 8.0/bin:/mnt/c/Program Files/MySQL/MySQL Shell 8.0/bin:/mnt/c/Program Files/yt-dlp:/mnt/c/Program Files (x86)/GnuPG/bin:/mnt/c/Program Files/Mullvad VPN/resources:/mnt/c/Program Files (x86)/Pulse Secure/VC142.CRT/X64/:/mnt/c/Program Files (x86)/Pulse Secure/VC142.CRT/X86/:/mnt/c/Program Files/Docker/Docker/resources/bin:/mnt/c/Strawberry/c/bin:/mnt/c/Strawberry/perl/site/bin:/mnt/c/Strawberry/perl/bin:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA NvDLISR:/mnt/c/Users/aesun/AppData/Local/Programs/Python/Python312/Scripts/:/mnt/c/Users/aesun/AppData/Local/Programs/Python/Python312/:/mnt/c/Users/aesun/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/aesun/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/aesun/AppData/Roaming/npm:/mnt/c/Users/aesun/AppData/Local/Programs/MiKTeX/miktex/bin/x64/:/mnt/c/Program Files/Neovim/bin:/snap/bin:/usr/local/cuda-12.4/bin\"\n",
    "#%env LD_LIBRARY_PATH=\"/home/aes/Datamining/.venv/lib/python3.10/site-packages/nvidia/cudnn/lib:/usr/local/cuda-12.4/lib64:/usr/local/cuda-12.4/extras/CUPTI/lib64:/usr/local/cuda-12.4/lib64\"\n",
    "#%env CUDNN_PATH=\"/home/aes/Datamining/.venv/lib/python3.10/site-packages/nvidia/cudnn\"\n",
    "\n",
    "# You should set up cudnn path in your system to get GPU to work\n",
    "# export CUDNN_PATH=\"/home/aes/Datamining/.venv/lib/python3.10/site-packages/nvidia/cudnn\"\n",
    "# and\n",
    "# export LD_LIBRARY_PATH=\"$CUDNN_PATH/lib\":\"/usr/local/cuda-12.4/lib64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aes/.local/lib/python3.10/site-packages/nvidia/cudnn/lib\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aes/.local/lib/python3.10/site-packages/nvidia/cudnn\n"
     ]
    }
   ],
   "source": [
    "# Make sure these two paths are set up correctly\n",
    "!echo $LD_LIBRARY_PATH\n",
    "!echo $CUDNN_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import urllib.request\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import json\n",
    "\n",
    "tf. __version__\n",
    "\n",
    "PRE_EVALUATE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique authors: 5\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('cleaned_data/uber_boat_cleaned.csv')\n",
    "data2 = pd.read_csv('cleaned_data/ch_cleaned.csv')\n",
    "\n",
    "# Concatenate the two datasets\n",
    "data = pd.concat([data, data2])\n",
    "author = data['author']\n",
    "message = data['message']\n",
    "header = data.columns\n",
    "\n",
    "# Count the number of unique authors\n",
    "author_id = author.unique()\n",
    "print('Number of unique authors:', len(author_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Set tensorflow debugging on or off\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "\n",
    "# Check if tensorflow is using the GPU\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# If you see num GPUs available as 0 then you should be worried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of message sequence: 250\n",
      "[1. 0. 0. 0. 0.]\n",
      "Sample size: 32126\n",
      "Number of messages per author before oversampling: {0: 2789, 1: 2235, 2: 13275, 3: 1568, 4: 12259}\n",
      "Number of messages per author after oversampling: {0: 13275, 1: 13275, 2: 13275, 3: 13275, 4: 13275}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the message\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(message)\n",
    "message_sequence = tokenizer.texts_to_sequences(message)\n",
    "\n",
    "# Pad the tokenized message sequence\n",
    "max_len = max([len(seq) for seq in message_sequence])\n",
    "print('Max length of message sequence:', max_len)\n",
    "message_sequence = tf.keras.utils.pad_sequences(message_sequence, maxlen=max_len)\n",
    "\n",
    "author_label = tf.keras.utils.to_categorical(author)\n",
    "print(author_label[0])\n",
    "\n",
    "# Count how many messages per author\n",
    "# First create a dictionary of authors and int\n",
    "author_id = author.unique() \n",
    "author_to_messages_count = {auth_id: 0 for auth_id in author_id}\n",
    "\n",
    "# Count the number of messages per author\n",
    "for a in author_label:\n",
    "    author_to_messages_count[np.argmax(a)] += 1\n",
    "\n",
    "print(\"Sample size:\", len(author_label))\n",
    "print('Number of messages per author before oversampling:', author_to_messages_count)\n",
    "\n",
    "# Oversample the dataset\n",
    "smote = SMOTE()\n",
    "message_os, author_os = smote.fit_resample(message_sequence, author_label)\n",
    "\n",
    "# Count the number of messages per author after oversampling\n",
    "author_to_messages_count = {auth_id: 0 for auth_id in author_id}\n",
    "for a in author_os:\n",
    "    author_to_messages_count[np.argmax(a)] += 1\n",
    "\n",
    "print('Number of messages per author after oversampling:', author_to_messages_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation and testing sets\n",
    "message_train, message_test, author_train, author_test = train_test_split(message_os, author_os, test_size=0.3, random_state=1, stratify=author_os)\n",
    "message_test, message_val, author_test, author_val = train_test_split(message_test, author_test, test_size=0.5, random_state=1, stratify=author_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "x = 256\n",
    "y = 128\n",
    "z = 64\n",
    "dropout_val = 0.5\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(len(tokenizer.word_index)+1, x, name='embedding'),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(y), name='lstm'),\n",
    "    tf.keras.layers.Dense(z, activation='relu', name='dense'),\n",
    "    tf.keras.layers.Dropout(dropout_val, name='dropout'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax', name='output')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRE_EVALUATE:\n",
    "    print(\"Model stats before training\")\n",
    "    test_pre_loss, test_pre_accuracy = model.evaluate(message_test, author_test)\n",
    "    print('Test loss:', test_pre_loss)\n",
    "    print(f\"Test accuracy: {test_pre_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1452/1452 [==============================] - 90s 60ms/step - loss: 1.5149 - accuracy: 0.3181 - val_loss: 1.3792 - val_accuracy: 0.4269\n",
      "Epoch 2/2\n",
      "1452/1452 [==============================] - 75s 52ms/step - loss: 1.1753 - accuracy: 0.5364 - val_loss: 1.2369 - val_accuracy: 0.4934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f857ebc2e60>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 2\n",
    "model.fit(message_train, author_train, epochs=epochs, validation_data=(message_val, author_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 7s 22ms/step - loss: 1.2431 - accuracy: 0.4890\n",
      "Test loss: 1.2431036233901978\n",
      "Test accuracy: 0.4889513850212097\n",
      "1452/1452 [==============================] - 32s 22ms/step - loss: 0.8679 - accuracy: 0.6776\n",
      "Recall loss: 0.8678728938102722\n",
      "Recall accuracy: 0.6775859594345093\n"
     ]
    }
   ],
   "source": [
    "# Test the model using the test data\n",
    "test_loss, test_accuracy = model.evaluate(message_test, author_test)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_accuracy)\n",
    "\n",
    "# Test recall\n",
    "recall_loss, recall_accuracy = model.evaluate(message_train, author_train)\n",
    "print('Recall loss:', recall_loss)\n",
    "print('Recall accuracy:', recall_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stop words\n",
    "STOP_WORDS = urllib.request.urlopen(\"https://github.com/igorbrigadir/stopwords/blob/master/en/postgresql.txt\").read().decode(\"utf-8\").split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_message(message):\n",
    "    message = message.replace(\".\", \"\")\n",
    "    message = message.replace(\",\", \"\")\n",
    "    message = message.replace(\";\", \"\")\n",
    "    message = message.replace(\"!\", \"\")\n",
    "    message = message.replace(\"?\", \"\")\n",
    "    message = message.replace(\"(\", \"\")\n",
    "    message = message.replace(\")\", \"\")\n",
    "    message = message.replace(\"\\\\\", \"\")\n",
    "    message = message.replace(\"\\\"\", \"\")\n",
    "\n",
    "    # remove discord effects\n",
    "    message = message.replace(\"*\", \"\")\n",
    "    message = message.replace(\"_\", \"\")\n",
    "    message = message.replace(\"~\", \"\")\n",
    "    message = message.replace(\"`\", \"\")\n",
    "    message = message.replace(\">\", \"\")\n",
    "    message = message.replace(\"<\", \"\")\n",
    "    message = message.replace(\"||\", \"\")\n",
    "    message = message.replace(\"```\", \"\")\n",
    "    message = message.replace(\"~~\", \"\")\n",
    "    message = message.replace(\":\", \"\")\n",
    "    message = message.replace(\"#\", \"\")\n",
    "    message = message.replace(\"@\", \"\")\n",
    "\n",
    "\n",
    "    # remove stopwords\n",
    "    message = message.lower()\n",
    "    message = ' '.join([word for word in message.split() if word not in STOP_WORDS or word in USER_NAMES])\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "participating_users = {}\n",
    "with open('users.json') as users_file:\n",
    "    temp = json.load(users_file)\n",
    "    participating_users = temp['participants']\n",
    "\n",
    "users = list(participating_users.keys())\n",
    "\n",
    "def predict_string_author(test_message_raw):\n",
    "    # remove stopwords\n",
    "    test_message = clean_message(test_message_raw)\n",
    "    test_message_sequence = tokenizer.texts_to_sequences([test_message])\n",
    "    test_message_sequence = tf.keras.utils.pad_sequences(test_message_sequence, maxlen=max_len)\n",
    "    prediction = model.predict(test_message_sequence)\n",
    "    preds = []\n",
    "    for i, name in enumerate(users):\n",
    "        preds.append([name, prediction[0][i] * 100])\n",
    "    \n",
    "    return users[np.argmax(prediction)], prediction[0][np.argmax(prediction)] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 500ms/step\n",
      "Message: Nyaa UwU I'm a femboy\n",
      "Predicted Author: Shrimpy Raccoon (41.82%)\n",
      "Actual Author: None\n",
      "\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Message: i'm going to become physically violent\n",
      "Predicted Author: jeremy (46.83%)\n",
      "Actual Author: None\n",
      "\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Message: kill sedimentary fuck metamorphic marry igneous\n",
      "Predicted Author: Stella (98.82%)\n",
      "Actual Author: None\n",
      "\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Message: kill me already\n",
      "Predicted Author: matt_m_h (31.37%)\n",
      "Actual Author: None\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Message: face planted into the corner of a table\n",
      "Predicted Author: jeremy (28.86%)\n",
      "Actual Author: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_message_raw = [\n",
    "    [\"Nyaa UwU I'm a femboy\", None],\n",
    "    [\"i'm going to become physically violent\", None],\n",
    "    [\"kill sedimentary fuck metamorphic marry igneous\", None],\n",
    "    [\"kill me already\", None],\n",
    "    [\"face planted into the corner of a table\", None]\n",
    "    ]\n",
    "\n",
    "for test_message, actual_author in test_message_raw:\n",
    "    username, prediction = predict_string_author(test_message)\n",
    "    print(f\"Message: {test_message}\\nPredicted Author: {username} ({prediction:.2f}%)\\nActual Author: {actual_author}\\n{'!!! CORRECT !!!' if actual_author == username else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as uber_boat_tf_2.15_1_model.keras\n",
      "Weights saved as uber_boat_tf_2.15_1_weights.weights.h5\n",
      "Tokenizer saved as uber_boat_tf_2.15_1_tokenizer.json\n",
      "Model summary saved as uber_boat_tf_2.15_1_model_summary.txt\n"
     ]
    }
   ],
   "source": [
    "model_name = \"uber_boat_tf_2.15\"\n",
    "file_extension = \".keras\"\n",
    "\n",
    "# Check if the model already exists if it already does add a number after it\n",
    "if os.path.exists(model_name + '_model' + file_extension):\n",
    "    model_name = model_name + '_1'\n",
    "    while os.path.exists(model_name + '_model' + file_extension):\n",
    "        model_name = model_name[:-1] + str(int(model_name[-1]) + 1)\n",
    "\n",
    "# Save the model\n",
    "model.save(model_name + '_model' + file_extension)\n",
    "\n",
    "model.save_weights(model_name + '.weights.h5')\n",
    "\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open(model_name + '_tokenizer.json', 'w') as json_file:\n",
    "    json_file.write(tokenizer_json)\n",
    "\n",
    "# Output the model summary\n",
    "with open(model_name + '_model_summary.txt', 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    f.write(f\"Epochs: {epochs}\\n\\n\")\n",
    "    f.write(f\"Hyperparameters: x={x}, y={y}, z={z}, dropout={dropout_val}\\n\")\n",
    "    f.write(f\"Testing Summary: \\n\")\n",
    "    f.write(f\"Test loss: {test_loss}\\n\")\n",
    "    f.write(f\"Test accuracy: {test_accuracy}\\n\")\n",
    "    f.write(f\"Recall loss: {recall_loss}\\n\")\n",
    "    f.write(f\"Recall accuracy: {recall_accuracy}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Model files saved as: \\n\")\n",
    "    f.write(f\"Model saved as {model_name}_model{file_extension}\\n\")\n",
    "    f.write(f\"Weights saved as {model_name}_weights.weights.h5\\n\")\n",
    "    f.write(f\"Tokenizer saved as {model_name}_tokenizer.json\\n\")\n",
    "\n",
    "print(f\"Model saved as {model_name}_model{file_extension}\")\n",
    "print(f\"Weights saved as {model_name}_weights.weights.h5\")\n",
    "print(f\"Tokenizer saved as {model_name}_tokenizer.json\")\n",
    "print(f\"Model summary saved as {model_name}_model_summary.txt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
